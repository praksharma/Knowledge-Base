{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP (OpenAI)\n",
    "CLIP was trained on a variety of image-text pairs, using a contrastive loss to learn to predict which images go with which texts. This is a zero-shot version of CLIP, where the model has not seen any examples of the tasks at hand. The model is able to predict which images correspond to the text prompts provided. The text being the prediction of the class of the image (hopefully).\n",
    "\n",
    "We are doing this because models train on imagenet dataset failed to predict the class of the image. So we are using CLIP model to predict the class of the image. It was trained on 400 million image-text pairs.\n",
    "\n",
    "We can install clip using the following command:\n",
    "```python\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "```\n",
    "\n",
    "## A key difference from traditional CNN\n",
    "CLIP matches images to texts. So, you provide an image and a text prompt, and it tells you how well they match. This is different from a traditional CNN, which predicts a class label for an image. CLIP can be used to predict class labels, but it requires a text prompt to do so. the text prompt can be anything. This is called zero-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device) # 300MB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image\n",
    "image = preprocess(Image.open(\"car.jpg\")).unsqueeze(0).to(device)\n",
    "prompts = [\"a vintage car\", \"a pickup truck\", \"a convertible\"]\n",
    "text = clip.tokenize(prompts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    logits_per_image, _ = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 0.8706 - tensor([49406,   320,  3266,  1615, 49407,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "2. 0.0621 - tensor([49406,   320, 15382,  4629, 49407,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "3. 0.0671 - tensor([49406,   320, 19608, 49407,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0], device='cuda:0',\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(text):\n",
    "    print(f\"{i+1}. {probs[0][i].item():.4f} - {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the first number after each token number is the similarity score of the class of the image. We can convert them to human readable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. a vintage car (0.8706)\n",
      "2. a pickup truck (0.0621)\n",
      "3. a convertible (0.0671)\n"
     ]
    }
   ],
   "source": [
    "for i, prob in enumerate(probs[0]):\n",
    "    print(f\"{i+1}. {prompts[i]} ({prob:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
