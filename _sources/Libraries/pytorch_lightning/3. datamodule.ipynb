{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53f28c1",
   "metadata": {},
   "source": [
    "# PyTorch Lightning: Data module and test dataset\n",
    "The PyTorch Lightning data module is a standard way to organize your data loading code. It helps you separate the data preparation from the model training code, making your code cleaner and more maintainable.\n",
    "\n",
    "A typical data module looks like this:\n",
    "\n",
    "```python\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download or prepare your data here\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Load your data here\n",
    "        self.train_dataset = MyDataset(train=True)\n",
    "        self.val_dataset = MyDataset(train=False)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "```\n",
    "\n",
    "\n",
    "The only change in the `Trainer` will be:\n",
    "\n",
    "```python\n",
    "trainer.fit(model, datamodule=dm)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9b8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import torchmetrics # for metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e43f15",
   "metadata": {},
   "source": [
    "## Data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff30ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(L.LightningDataModule):\n",
    "    def __init__(self, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        MNIST(root=\"./MNIST\", train=True, download=True)\n",
    "        MNIST(root=\"./MNIST\", train=True, download=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            full = MNIST(root=\"./MNIST\", train=True, transform= self.transform)\n",
    "            self.train, self.val = random_split(full, [55000, 5000])\n",
    "        \n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test = MNIST(root=\"./MNIST\", train=False, transform= self.transform)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size= self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size= self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size= self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0417cec",
   "metadata": {},
   "source": [
    "## NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf8ce1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(L.LightningModule): # a replacesment of nn.Module\n",
    "    def __init__(self):\n",
    "      super().__init__() # call __init__ of the super class to init important LightningModule functions\n",
    "      self.model = nn.Sequential(\n",
    "         nn.Flatten(),\n",
    "         nn.Linear(28*28, 128),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(128, 10)\n",
    "      )\n",
    "\n",
    "      self.train_acc = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "      self.val_acc = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "      self.test_acc = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "    def forward(self, x):\n",
    "      return self.model(x)\n",
    "   \n",
    "    def training_step(self, batch, batch_idx):\n",
    "      x, y = batch\n",
    "      logits = self(x)\n",
    "      loss = F.cross_entropy(logits, y)\n",
    "      acc = self.train_acc(logits, y)\n",
    "      self.log(\"train_loss\", loss)\n",
    "      self.log(\"train_acc\", acc, prog_bar=True)\n",
    "      #return loss\n",
    "      return {\"loss\": loss} #both are the same\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = self.val_acc(logits, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True) # prog_bar=False will not show the val loss in the training progress bar.\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = self.test_acc(logits, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "      return torch.optim.Adam(self.parameters(), lr=1e-3) # the NN get the parameters not self.model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39dbb8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | Sequential         | 101 K  | train\n",
      "1 | train_acc | MulticlassAccuracy | 0      | train\n",
      "2 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "3 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hell/Desktop/lightning/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/hell/Desktop/lightning/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 860/860 [00:15<00:00, 55.43it/s, v_num=4, train_acc=0.958, val_loss=0.124, val_acc=0.965]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 860/860 [00:15<00:00, 55.40it/s, v_num=4, train_acc=0.958, val_loss=0.124, val_acc=0.965]\n"
     ]
    }
   ],
   "source": [
    "# Datamodule\n",
    "dm = MNISTDataModule()\n",
    "# checkpoint based on val loss\n",
    "checkpoint_cb = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "\n",
    "# trainer\n",
    "model = LitModel()\n",
    "trainer = L.Trainer(max_epochs = 3,\n",
    "                    accelerator=\"auto\", # auto will select gpu if available\n",
    "                    callbacks=[checkpoint_cb]) \n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f8077b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/hell/Desktop/lightning/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 80.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9667999744415283     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.11054354161024094    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9667999744415283    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11054354161024094   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.11054354161024094, 'test_acc': 0.9667999744415283}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
