{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Models: Vision Transformers (ViTs)\n",
    "They are a newer architecture that brings in the power of transformers to computer vision. They have shown to be very effective in image classification tasks. They are trained on the ImageNet dataset so we can use the same class labels from the PyTorch's Hub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a sample image\n",
    "img = Image.open(\"car.jpg\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit_b_16(pretrained=True).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the image through the model\n",
    "img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(img_tensor)\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the class labels\n",
    "# Download the txt file with human-readable labels\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "filename = \"imagenet_classes.txt\"\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# Load labels\n",
    "with open(filename) as f:\n",
    "    labels = [line.strip() for line in f.readlines()] # read all labels line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. beach wagon (0.2748)\n",
      "2. pickup (0.2295)\n",
      "3. grille (0.1784)\n",
      "4. convertible (0.1343)\n",
      "5. cab (0.0189)\n"
     ]
    }
   ],
   "source": [
    "top5 = torch.topk(probs, 5) # get the top 5 probabilities and their indices\n",
    "for i in range(5):\n",
    "    class_id = top5.indices[0][i].item() # get the index of the top 5 classes\n",
    "    score = top5.values[0][i].item() # get the score of the top 5 classes\n",
    "    print(f\"{i+1}. {labels[class_id]} ({score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imagenet dataset was limited to 1000 classes, so our output is inaccurate. We will need to find a pretrained model that was trained on the dataset we are interested in or on a bigger dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
